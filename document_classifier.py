import re
import json
from typing import Dict, List, Union, Optional, Tuple
from pathlib import Path

class TableOfContents:
    def __init__(self):
        self.entries = []  # [{title, page_number, pattern}]
        self.toc_page_range = None  # (start_page, end_page)
    
    def extract_toc_from_json(self, document: Dict, toc_page_range: Tuple[int, int]):
        """
        JSON ë¬¸ì„œì—ì„œ ëª©ì°¨ í˜ì´ì§€ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        self.toc_page_range = toc_page_range
        toc_items = []
        
        # elements ë°°ì—´ì—ì„œ ëª©ì°¨ í˜ì´ì§€ì˜ í•­ëª©ë“¤ë§Œ ì¶”ì¶œ
        for element in document.get('elements', []):
            page = element.get('page', 0)
            if not (toc_page_range[0] <= page <= toc_page_range[1]):
                continue
                
            content = element.get('content', {})
            text = content.get('text', '') or content.get('markdown', '')
            if not text:
                continue
            
            # ê° ì¤„ë³„ë¡œ ì²˜ë¦¬
            for line in text.split('\n'):
                line = line.strip()
                if not line or len(line) < 3:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                    continue
                
                # ëª©ì°¨ í•­ëª© íŒ¨í„´ (ì˜ˆ: "01. ë¹„ê³¼ì„¸ - ìê¸°ì°¨ëŸ‰ ìš´ì „ë³´ì¡°ê¸ˆ ... 15")
                # í˜ì´ì§€ ë²ˆí˜¸ê°€ ëì— ìˆëŠ” íŒ¨í„´ ë§¤ì¹­
                match = re.search(r'^[-\s>]*([^.]+(?:\.[^.]+)*?)(?:[.]{2,}|\s{3,}|[.Â·]{1}\s+|\s*[.]{3,}\s*|\s+[Â·]\s+|\s+[>]+\s+)(\d+)\s*$', line.strip())
                if match:
                    title, target_page = match.groups()
                    title = title.strip()
                    target_page = int(target_page)
                    
                    # ì œëª©ì—ì„œ ë²ˆí˜¸ ë¶€ë¶„ ì¶”ì¶œ
                    number_match = re.match(r'^[-\s>]*(\d{2}\.|\([0-9]+\)|[IVX]+\.|[A-Z]\.)', title)
                    if number_match:
                        number_part = number_match.group(1)
                        # ì œëª© íŒ¨í„´ ìƒì„± (ë²ˆí˜¸ ë¶€ë¶„ì€ ì„ íƒì ìœ¼ë¡œ í¬í•¨)
                        title_pattern = title.replace(number_part, f"({number_part})?")
                    else:
                        title_pattern = title
                    
                    # ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ë³€í™˜
                    pattern = re.escape(title_pattern).replace(r"\ ", r"\s+")
                    
                    toc_items.append({
                        'title': title,
                        'target_page': target_page,  # ì‹¤ì œ ë‚´ìš©ì´ ìˆëŠ” í˜ì´ì§€ ë²ˆí˜¸
                        'pattern': pattern,
                        'coordinates': element.get('coordinates', [])
                    })
        
        # í˜ì´ì§€ ë²ˆí˜¸ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        self.entries = sorted(toc_items, key=lambda x: x['target_page'])
    
    def save_toc_list(self, output_path: str):
        """
        ëª©ì°¨ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# ëª©ì°¨ ë¦¬ìŠ¤íŠ¸\n\n")
            for entry in self.entries:
                f.write(f"- {entry['title']} (í˜ì´ì§€: {entry['target_page']})\n")
    
    def is_title(self, text: str, page: int, coordinates: List[Dict] = None) -> bool:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ê°€ ëª©ì°¨ì— ìˆëŠ” ì œëª©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        text = text.strip()
        
        # í…ìŠ¤íŠ¸ ë§¤ì¹­ ë° í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸
        for entry in self.entries:
            if re.search(entry['pattern'], text, re.IGNORECASE):
                # í˜ì´ì§€ ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                if entry['target_page'] == page:
                    # ì¢Œí‘œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ê²€ì¦
                    if coordinates and entry.get('coordinates'):
                        if self._check_coordinate_similarity(coordinates, entry['coordinates']):
                            return True
                    else:
                        return True
        return False
    
    def _check_coordinate_similarity(self, coords1: List[Dict], coords2: List[Dict]) -> bool:
        """
        ë‘ ì¢Œí‘œ ì§‘í•©ì˜ ìœ ì‚¬ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        """
        if not coords1 or not coords2:
            return True
            
        # x ì¢Œí‘œì˜ ìœ ì‚¬ì„± ê²€ì‚¬ (ì™¼ìª½ ì •ë ¬ ì—¬ë¶€)
        x1 = coords1[0].get('x', 0)
        x2 = coords2[0].get('x', 0)
        
        return abs(x1 - x2) < 0.1  # 10% ì´ë‚´ì˜ ì°¨ì´ëŠ” í—ˆìš©

class DocumentClassifier:
    def __init__(self, toc: TableOfContents):
        self.toc = toc
        self.headings = []  # ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ëª¨ë“  ì œëª©
    
    def classify_content(self, element: Dict) -> str:
        """
        ì½˜í…ì¸ ë¥¼ ì œëª© ë˜ëŠ” ë³¸ë¬¸ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
        """
        page = element.get('page', 0)
        content = element.get('content', {})
        text = content.get('text', '') or content.get('markdown', '')
        if not text or len(text.strip()) < 3:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
            return 'unknown'
            
        coordinates = element.get('coordinates', [])
        
        # ëª©ì°¨ í˜ì´ì§€ ë²”ìœ„ì— ìˆëŠ” ê²½ìš°
        if self.toc.toc_page_range and self.toc.toc_page_range[0] <= page <= self.toc.toc_page_range[1]:
            # íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” í…ìŠ¤íŠ¸ë§Œ ì œëª©ìœ¼ë¡œ ì²˜ë¦¬
            if self._is_potential_title(text, coordinates):
                self.headings.append({
                    'text': text,
                    'page': page,
                    'source': 'toc_page'
                })
                return 'heading'
            return 'toc'
            
        # ëª©ì°¨ ê¸°ë°˜ ì œëª© í™•ì¸ (í˜ì´ì§€ ë²ˆí˜¸ë„ í•¨ê»˜ í™•ì¸)
        if self.toc.is_title(text, page, coordinates):
            self.headings.append({
                'text': text,
                'page': page,
                'source': 'toc_match'
            })
            return 'heading'
            
        # categoryê°€ headingìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì œëª©ìœ¼ë¡œ ë¶„ë¥˜
        category = element.get('category', '')
        if category and category.startswith('heading'):
            self.headings.append({
                'text': text,
                'page': page,
                'source': 'category'
            })
            return 'heading'
            
        return 'paragraph'
    
    def _is_potential_title(self, text: str, coordinates: List[Dict]) -> bool:
        """
        í…ìŠ¤íŠ¸ê°€ ì œëª©ì¼ ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        text = text.strip()
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
        if len(text) > 200:
            return False
            
        # íŠ¹ìˆ˜ë¬¸ìë‚˜ ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
        if re.match(r'^[0-9\s\.\-\=\>\<\(\)]+$', text):
            return False
            
        # ì¢Œí‘œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, ì™¼ìª½ ì •ë ¬ ì—¬ë¶€ í™•ì¸
        if coordinates and len(coordinates) > 0:
            x = coordinates[0].get('x', 0)
            if x > 0.5:  # í˜ì´ì§€ ì¤‘ê°„ ì´í›„ì— ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                return False
        
        # ì œëª© íŒ¨í„´ í™•ì¸
        title_patterns = [
            r'^\d+\.',  # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´
            r'^[IVX]+\.',  # ë¡œë§ˆ ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´
            r'^[A-Z]\.',  # ëŒ€ë¬¸ì ì•ŒíŒŒë²³ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´
            r'^ì œ\d+',  # 'ì œ'ë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´
            r'^[â– â–¶â€»â—â—‹â—†â–¡â–³â–²â–¼]',  # íŠ¹ìˆ˜ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´
            r'[ê°€-í£]+\s*[í¸ì¥ì ˆí•­ëª©]$'  # í•œê¸€ + í¸/ì¥/ì ˆ/í•­/ëª©ìœ¼ë¡œ ëë‚˜ëŠ” íŒ¨í„´
        ]
        
        for pattern in title_patterns:
            if re.search(pattern, text):
                return True
        
        return False
    
    def save_heading_list(self, output_path: str):
        """
        ë°œê²¬ëœ ëª¨ë“  ì œëª©ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# ë¬¸ì„œ ì œëª© ë¦¬ìŠ¤íŠ¸\n\n")
            for heading in sorted(self.headings, key=lambda x: (x['page'], x['text'])):
                source_mark = {
                    'toc_match': 'ğŸ“š',
                    'category': 'ğŸ“',
                    'toc_page': 'ğŸ“–'
                }.get(heading['source'], 'â“')
                f.write(f"{source_mark} {heading['text']} (í˜ì´ì§€: {heading['page']})\n")

def process_document(document: Dict, toc_page_range: Tuple[int, int]) -> Tuple[Dict, TableOfContents, DocumentClassifier]:
    """
    ëª©ì°¨ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # ëª©ì°¨ íŒŒì‹±
    toc = TableOfContents()
    toc.extract_toc_from_json(document, toc_page_range)
    
    # ë¬¸ì„œ ë¶„ë¥˜
    classifier = DocumentClassifier(toc)
    classified_elements = []
    
    for element in document.get('elements', []):
        content_type = classifier.classify_content(element)
        classified_element = {
            **element,
            'type': content_type
        }
        classified_elements.append(classified_element)
    
    # ì›ë³¸ ë¬¸ì„œ êµ¬ì¡° ìœ ì§€
    classified_document = {
        **document,
        'elements': classified_elements
    }
    
    return classified_document, toc, classifier

def process_json_file(input_path: str, toc_page_range: Tuple[int, int], output_path: str = None):
    """
    JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ìƒˆ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    input_path = Path(input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        
    if output_path is None:
        output_path = input_path.parent / f"{input_path.stem}_classified{input_path.suffix}"
    
    # JSON íŒŒì¼ ì½ê¸°
    with open(input_path, 'r', encoding='utf-8') as f:
        document_data = json.load(f)
    
    # ë¬¸ì„œ ì²˜ë¦¬
    classified_document, toc, classifier = process_document(document_data, toc_page_range)
    
    # ê²°ê³¼ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(classified_document, f, ensure_ascii=False, indent=2)
    
    # ëª©ì°¨ì™€ ì œëª© ë¦¬ìŠ¤íŠ¸ ì €ì¥
    toc_list_path = input_path.parent / f"{input_path.stem}_toc_list.md"
    heading_list_path = input_path.parent / f"{input_path.stem}_heading_list.md"
    
    toc.save_toc_list(toc_list_path)
    classifier.save_heading_list(heading_list_path)
    
    # í†µê³„ ì¶œë ¥
    elements = classified_document.get('elements', [])
    print(f"\nì „ì²´ í•­ëª© ìˆ˜: {len(elements)}")
    print(f"ë¶„ë¥˜ëœ í•­ëª©:")
    type_counts = {}
    for element in elements:
        type_counts[element['type']] = type_counts.get(element['type'], 0) + 1
    for type_name, count in sorted(type_counts.items()):
        print(f"- {type_name}: {count}ê°œ")
    
    print(f"\nëª©ì°¨ ë¦¬ìŠ¤íŠ¸ ì €ì¥ë¨: {toc_list_path}")
    print(f"ì œëª© ë¦¬ìŠ¤íŠ¸ ì €ì¥ë¨: {heading_list_path}")
    
    return output_path

if __name__ == "__main__":
    # íŒŒì¼ ì²˜ë¦¬ ì˜ˆì‹œ
    input_file = "ì—°ë§ì •ì‚°_1_14.json"
    output_file = "ì—°ë§ì •ì‚°_1_14_classified.json"
    
    # ëª©ì°¨ í˜ì´ì§€ ë²”ìœ„ ì§€ì • (4-8 í˜ì´ì§€ê°€ ëª©ì°¨)
    toc_page_range = (4, 8)
    
    try:
        output_path = process_json_file(input_file, toc_page_range, output_file)
        print(f"\në¶„ë¥˜ ì™„ë£Œ. ê²°ê³¼ê°€ ì €ì¥ëœ íŒŒì¼: {output_path}")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}") 